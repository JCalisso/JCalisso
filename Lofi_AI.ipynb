{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "zGP1olZv4Xpc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCalisso/JCalisso/blob/main/Lofi_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iwSi8Setf4K"
      },
      "source": [
        "# Lofi? More like Lo-success. This isn't very good.\n",
        "\n",
        "Ah shit, here we go again - a coding project that is *way* more difficult than I expected and I expected it to be pretty difficult. Inspired by [this guide](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5) by Sigurður Skúli, let's try to use an LSTM to generate a jazzy lo-fi instrumental melody.\n",
        "\n",
        "**Goal:** Use machine learning to generate the fundamental building blocks of a lofi song (a drum beat and an jazzy instrumental). To be more precise, lofi music is usually 70-95 bpm with an authentic, organic, degraded, boom bap kinda sound (pioneered by J Dilla)\n",
        "\n",
        "If you want to play around with this notebook, just duplicate it to your own colab space or look through the code and steal stuff. Not sure it's worth stealing though. The output is rather garbage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioRJWTunC0Yj"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nWGqV-KpZQS",
        "cellView": "form"
      },
      "source": [
        "#@title Import Libraries\n",
        "#@markdown Some things you'll need to do the stuff in here.\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import music21 as m\n",
        "from google.colab import files\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc0NUnTvyGU4",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Functions\n",
        "#@markdown Just some functions because good coding practice is important or something. Idk. I'm really only doing this because I know it's going to be an exclusive and other people will see it and i don't want to be shamed.\n",
        "def read_midi(path, printout = False):\n",
        "  \"\"\"\n",
        "  Given a path to a midi file, return every note and duration. If print is True, it'll print out each note and duration.\n",
        "  \"\"\"\n",
        "  notes = []\n",
        "  dur = []\n",
        "  \n",
        "  # load midi data into stream objects\n",
        "  midi = m.converter.parse(path)\n",
        "  notes_to_parse = None    \n",
        "  parts = m.instrument.partitionByInstrument(midi)    \n",
        "  \n",
        "  if parts: # file has instrument parts\n",
        "    notes_to_parse = parts.parts[0].recurse()\n",
        "  else: # file has notes in a flat structure\n",
        "    notes_to_parse = midi.flat.notes    \n",
        "\n",
        "  # use stream data to convert to string\n",
        "  for e in notes_to_parse:\n",
        "    if isinstance(e, m.note.Note):\n",
        "        notes.append(str(e.pitch))\n",
        "        dur.append(float(e.duration.quarterLength))\n",
        "        if printout: \n",
        "          print(e.pitch, e.duration.quarterLength)\n",
        "    elif isinstance(e, m.chord.Chord):\n",
        "        notes.append('.'.join(str(n) for n in e.normalOrder))\n",
        "        dur.append(float(e.duration.quarterLength))\n",
        "        if printout: \n",
        "          print('.'.join(str(n) for n in e.normalOrder), e.duration.quarterLength)\n",
        "  return notes, dur\n",
        "\n",
        "def create_in_out(notes, dur, sequence_length):\n",
        "  \"\"\"\n",
        "  Given a list of notes, a list of durations, and a sequence length. This function returns a normalized input array \n",
        "  and two one-hot encoded output sequences (notes, durations) for our LSTM model.\n",
        "  \"\"\"\n",
        "  # get all pitch names\n",
        "  pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "  # create a dictionary to map pitches to integers (for the sake of normalization)\n",
        "  note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "  network_input = []\n",
        "  network_output = []\n",
        "\n",
        "  # create input sequences and the corresponding outputs\n",
        "  for i in range(0, len(notes) - sequence_length, 1):\n",
        "      sequence_in = notes[i:i + sequence_length]\n",
        "      sequence_in = [note_to_int[char] for char in sequence_in]\n",
        "      sequence_out = notes[i + sequence_length]\n",
        "      sequence_out = note_to_int[sequence_out]\n",
        "      dur_in = dur[i:i + sequence_length]\n",
        "      dur_out = dur[i + sequence_length]\n",
        "      network_input.append([(a,b) for a,b in zip(sequence_in, dur_in)])\n",
        "      network_output.append((sequence_out, dur_out))\n",
        "      \n",
        "  n_patterns = len(network_input)\n",
        "  network_input = np.asarray(network_input)\n",
        "  network_output = np.asarray(network_output)\n",
        "  \n",
        "  # normalize inputs\n",
        "  n_notes = len(set(notes))\n",
        "  n_dur = len(set(dur))\n",
        "  network_input[:,:,0] = network_input[:,:,0] / float(n_notes)\n",
        "  network_input[:,:,1] = network_input[:,:,1] / float(n_dur)\n",
        "\n",
        "  # one hot encode the note output information since it is categorical\n",
        "  note_encode = np_utils.to_categorical(network_output[:,0])\n",
        "\n",
        "  # one hot encode the duration output information since you can argue it's categorical (due to music theory or whatever lmao)\n",
        "  dur_encode = np_utils.to_categorical(network_output[:,1], num_classes=n_dur)\n",
        "\n",
        "  # reshape data for LSTM input\n",
        "  network_input = np.reshape(network_input,(n_patterns, sequence_length,2,1))\n",
        "\n",
        "  # make sure the shapes make sense\n",
        "  if (network_input.shape[0] !=  note_encode.shape[0]) | (network_input.shape[0] !=  dur_encode.shape[0]) | (dur_encode.shape[0] !=  note_encode.shape[0]):\n",
        "    print('ERROR: something is wrong with the shape of ur stuff. Go check out helper functions and see if you know what is going on.')\n",
        "\n",
        "  return network_input, note_encode, dur_encode\n",
        "\n",
        "def model_maker(sequence_length):\n",
        "  \"\"\"\n",
        "  Make a model. Mess around with this architecture because i truly head-empty copy-pasted this stuff from a tutorial basically.\n",
        "  \"\"\"\n",
        "  n_notes = len(set(notes))\n",
        "  n_dur = len(set(dur))\n",
        "\n",
        "  input_layer = Input(shape = (sequence_length,2,))\n",
        "  \n",
        "  encoder = LSTM(512, return_sequences=True)(input_layer)\n",
        "  \n",
        "  x = Dropout(0.3)(encoder)\n",
        "  x = LSTM(512, return_sequences=True)(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = LSTM(512)(x)\n",
        "\n",
        "  note_dense = Dense(256)(x)\n",
        "  note_decoder = Dropout(0.3)(note_dense)\n",
        "  note_out=Dense(n_notes, activation=\"softmax\")(note_decoder)\n",
        "\n",
        "  dur_dense = Dense(256)(x)\n",
        "  dur_dedcoder = Dropout(0.3)(dur_dense)\n",
        "  dur_out=Dense(n_dur, activation=\"softmax\")(dur_dedcoder)\n",
        "\n",
        "  model = Model(inputs=input_layer, outputs=[note_out,dur_out])\n",
        "  opt = RMSprop(learning_rate=0.01)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "  return model\n",
        "\n",
        "def model_fitter(model, network_input, note_encode, dur_encode, epochs=20, batch_size=64):\n",
        "  # train the model\n",
        "  filepath = \"weights-{epoch:02d}-{loss:.4f}.hdf5\"    \n",
        "  checkpoint = ModelCheckpoint(\n",
        "      filepath, monitor='loss', \n",
        "      verbose=0,        \n",
        "      save_best_only=True,        \n",
        "      mode='min'\n",
        "  ) \n",
        "  callbacks_list = [checkpoint]     \n",
        "  model.fit(network_input, [note_encode, dur_encode], \n",
        "            epochs=epochs, batch_size=batch_size, \n",
        "            callbacks = callbacks_list)\n",
        "  return None\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    The LSTM will always get caught in a loop of notes unless you do this sampling thing.\n",
        "    \"\"\"\n",
        "    preds = np.reshape(preds, preds.size)\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def predict_notes(network_input,notes,dur,n_gen, n_temp=0.15, d_temp=1):\n",
        "    \"\"\"\n",
        "    Generate n_gen notes. I have given up on good documentation. I am so tired.\n",
        "    This samples notes rather than choosing the argmax if you care about kinda stuff. \n",
        "    \"\"\"\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    durnames = sorted(set(item for item in dur))\n",
        "    n_notes = len(set(notes))\n",
        "    n_dur = len(set(dur)) \n",
        "\n",
        "    start = np.random.randint(0, len(network_input)-1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "    int_to_dur = dict((number, dur) for number, dur in enumerate(durnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate notes\n",
        "    for note_index in range(n_gen):\n",
        "\n",
        "        out = []\n",
        "\n",
        "        prediction_input = np.copy(pattern)\n",
        "        prediction_input = np.reshape(prediction_input, (1, len(prediction_input), 2))\n",
        "        prediction_input[0,:,0] = prediction_input[0,:,0] / float(n_notes)\n",
        "        prediction_input[0,:,1] = prediction_input[0,:,1] / float(n_dur)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "        # identify the most likely note in the set of notes\n",
        "        note_index = sample(prediction[0], n_temp)\n",
        "        note_result = int_to_note[note_index]\n",
        "        out.append(note_result)\n",
        "        # identify the most likely duration in the set of notes\n",
        "        dur_index = sample(prediction[1], d_temp)\n",
        "        dur_result = int_to_dur[dur_index]\n",
        "        out.append(dur_result)\n",
        "        prediction_output.append(out)\n",
        "        pattern = np.append(pattern, [note_index, dur_index])\n",
        "        pattern = np.reshape(pattern, (1, int(len(pattern)/2), 2))\n",
        "        pattern = pattern[0,1:pattern.shape[1]]\n",
        "    return prediction_output\n",
        "\n",
        "def repeat_agg(preds, note_extender):\n",
        "  \"\"\"\n",
        "  Helper function that takes in a list of lists of notes and duration,\n",
        "  It returns an ndarray where any repeated notes have their durations agregated.\n",
        "  \"\"\"\n",
        "  new_preds = []\n",
        "  # replace grace notes (ones with zero duration) with 0.25 duration\n",
        "  for i in range(len(preds)):\n",
        "    preds[i][1] += note_extender\n",
        "\n",
        "  # convert to a pandas dataframe to simplify aggregation\n",
        "  df = pd.DataFrame(preds, columns = ['note','dur'])\n",
        "  df = df.groupby(df.note.ne(df.note.shift()).cumsum()).agg({'note':'first','dur':'sum'}).reset_index(drop=True)\n",
        "\n",
        "  # convert back into a list\n",
        "  df = np.asarray(df)\n",
        "  for e in df:\n",
        "    new_preds.append(list(e))\n",
        "  return new_preds\n",
        "\n",
        "def create_midi(prediction_output, midi_name, note_extender = 4, offset_increase = 1):\n",
        "  \"\"\"\n",
        "  Given prediction output, generate a midi file.\n",
        "  \"\"\"\n",
        "  offset = 0\n",
        "  # create note and chord objects based on the values generated by the model\n",
        "  output_notes = []\n",
        "\n",
        "  # offset as appropriate\n",
        "  for pattern in repeat_agg(prediction_output, note_extender):\n",
        "    n = pattern[0]\n",
        "    d = float(pattern[1])\n",
        "    # pattern is a chord\n",
        "    if ('.' in n) or n.isdigit():\n",
        "        notes_in_chord = n.split('.')\n",
        "        notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = m.note.Note(int(current_note), quarterLength=d)\n",
        "            new_note.storedInstrument = m.instrument.Piano()\n",
        "            notes.append(new_note)\n",
        "        new_chord = m.chord.Chord(notes)\n",
        "        new_chord.offset = offset\n",
        "        output_notes.append(new_chord)\n",
        "    # pattern is a note\n",
        "    else:\n",
        "        new_note = m.note.Note(n, quarterLength = d)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = m.instrument.Piano()\n",
        "        output_notes.append(new_note)    # increase offset each iteration so that notes do not stack\n",
        "    offset += offset_increase\n",
        "\n",
        "  midi_stream = m.stream.Stream(output_notes)\n",
        "  mf = midi_stream.write('midi', fp='{}.mid'.format(midi_name))\n",
        "  \n",
        "  return mf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo6N8_Ug4pjL"
      },
      "source": [
        "## Make Your Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKyJroqx61F2",
        "cellView": "form"
      },
      "source": [
        "#@title Data Preperation\n",
        "#@markdown What to do:\n",
        "#@markdown 1. Upload single instrument MIDI samples to your Colab notebook (keep in mind that these get deleted after every session. If you have a bunch, it might be worth mounting your drive and saving a path). Theoretically, the more samples you have, the better your output will sound.\n",
        "#@markdown 2. Read your midi samples and concatenate the results into two lists: one for notes, another for durations.\n",
        "#@markdown 3. Create model inputs and outputs based on `sequence_length`. You should play around with the sequence length depending on your midi imports. Longer sequences might give your machine the opportunity to learn more structural patterns. Maybe.\n",
        "\n",
        "path = \"\" #@param {type:\"string\"}\n",
        "sequence_length =  16#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Do you want to see the notes as they're read?\n",
        "printout = False #@param {type:\"boolean\"}\n",
        "\n",
        "notes = []\n",
        "dur = []\n",
        "# read every midi file in your path\n",
        "for file in glob.glob(path+'/*.mid'):\n",
        "  r = read_midi(file, printout = printout)\n",
        "  notes += r[0]\n",
        "  dur += r[1]\n",
        "\n",
        "network_input, note_encode, dur_encode = create_in_out(notes, dur, sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_EeAEbOEsHq",
        "cellView": "form"
      },
      "source": [
        "#@title Train your Model\n",
        "#@markdown The following code will upload parameters as their quality improves (measured by categorical crossentropy).\n",
        "#@markdown Once training is down, you should probably download the best set of parameters so you don't need to train the model every time you want to make predictions.\n",
        "#@markdown\n",
        "#@markdown Also worth heading into notebook settings and getting a GPU accelerator to make this process a little less miserable.\n",
        "#@markdown\n",
        "#@markdown You can google what these mean but basically training time increases when epoch goes up and/or batch size goes down\n",
        "epochs = 30 #@param {type:\"slider\", min:10, max:100, step:10}\n",
        "batch_size = 64 #@param {type:\"slider\", min:12, max:128, step:4}\n",
        "model = model_maker(sequence_length)\n",
        "model_fitter(model, network_input, note_encode, dur_encode, epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlG0GBzCGsZ-"
      },
      "source": [
        "## Make Some Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_x1kLEwHZTP",
        "cellView": "form"
      },
      "source": [
        "#@title Load model\n",
        "#@markdown If you have a .hdf5 file of model weights from previous training, drop that path here.\n",
        "#@markdown If you don't, train your model.\n",
        "#@markdown\n",
        "#@markdown Make sure you haven't altered the `model_maker` function or `sequence_length` parameter between training and loading otherwise the modedl weights won't make sense.\n",
        "weight_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# build model\n",
        "model = model_maker(sequence_length)\n",
        "\n",
        "# load weight from training\n",
        "model.load_weights(weight_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e836bl8YBrEd",
        "cellView": "form"
      },
      "source": [
        "#@title Make Some Music\n",
        "#@markdown Adjust the following settings, run the code block, and you'll be prompted to download a MIDI file of your sweet, sweet tunes.\n",
        "#\n",
        "#@markdown Higher temperature will make predictions whackier.\n",
        "note_temperature = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "duration_temperature = 5 #@param {type:\"slider\", min:0, max:5, step:0.5}\n",
        "#@markdown How many notes do you want to generate?\n",
        "n_gen = 46 #@param {type:\"slider\", min:16, max:500, step:10}\n",
        "#@markdown What do you want your midi file to be named?\n",
        "midi_name = \"drum loop\" #@param {type:\"string\"}\n",
        "\n",
        "prediction_output = predict_notes(network_input,notes,dur,n_gen, n_temp=note_temperature, d_temp=duration_temperature)\n",
        "mf = create_midi(prediction_output, midi_name, note_extender=note_extender, offset_increase = offset_increase)\n",
        "\n",
        "files.download(mf) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGP1olZv4Xpc"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehd3lh063fmS",
        "cellView": "code"
      },
      "source": [
        "#@title Archived model training\n",
        "#@markdown This is some of the code that I used while building this thing out. It's basically just a bunch of things that should've been helper functions in the first place if I had more foresight. But unfortunately, I need glasses.\n",
        "\n",
        "# load midi data into stream objects\n",
        "notes = [] \n",
        "#offset = []\n",
        "dur = []\n",
        "\n",
        "for file in glob.glob(\"/content/samples/*.mid\"):\n",
        "    midi = converter.parse(file)\n",
        "    notes_to_parse = None    \n",
        "    parts = instrument.partitionByInstrument(midi)    \n",
        "    if parts: # file has instrument parts\n",
        "        notes_to_parse = parts.parts[0].recurse()\n",
        "    else: # file has notes in a flat structure\n",
        "        notes_to_parse = midi.flat.notes    \n",
        "\n",
        "    # use stream data to convert to string\n",
        "    for e in notes_to_parse:\n",
        "        if isinstance(e, note.Note):\n",
        "            notes.append(str(e.pitch))\n",
        "#            offset.append(e.offset)\n",
        "            dur.append(e.duration.quarterLength)\n",
        "        elif isinstance(e, chord.Chord):\n",
        "            notes.append('.'.join(str(n) for n in e.normalOrder))\n",
        "#            offset.append(e.offset)\n",
        "            dur.append(e.duration.quarterLength)\n",
        "\n",
        "dur = np.asarray(dur, dtype = 'float32')\n",
        "\n",
        "# map string data to integer data for ML\n",
        "sequence_length = 50\n",
        "\n",
        "# get all pitch names\n",
        "pitchnames = sorted(set(item for item in notes))\n",
        "durnames = sorted(set(item for item in dur))\n",
        "\n",
        "# create a dictionary to map pitches to integers (for the sake of normalization)\n",
        "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "network_input = []\n",
        "network_output = []\n",
        "\n",
        "# create input sequences and the corresponding outputs\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "    sequence_in = notes[i:i + sequence_length]\n",
        "    sequence_in = [note_to_int[char] for char in sequence_in]\n",
        "    sequence_out = notes[i + sequence_length]\n",
        "    sequence_out = note_to_int[sequence_out]\n",
        "    dur_in = dur[i:i + sequence_length]\n",
        "    dur_out = dur[i + sequence_length]\n",
        "    network_input.append([(a,b) for a,b in zip(sequence_in, dur_in)])\n",
        "    network_output.append((sequence_out, dur_out))\n",
        "    \n",
        "n_patterns = len(network_input)\n",
        "network_input = np.asarray(network_input)\n",
        "network_output = np.asarray(network_output)\n",
        "\n",
        "# normalize inputs\n",
        "n_notes = len(set(notes))\n",
        "n_dur = len(set(dur))\n",
        "network_input[:,:,0] = network_input[:,:,0] / float(n_notes)\n",
        "\n",
        "#network_input[:,1,:] = network_input[:,1,:] / float(n_offset)\n",
        "network_input[:,:,1] = network_input[:,:,1] / float(n_dur)\n",
        "\n",
        "# one hot encode the note output information since it is categorical\n",
        "note_encode = np_utils.to_categorical(network_output[:,0])\n",
        "\n",
        "# one hot encode the duration output information since you can argue it's categorical (due to music theory or whatever lmao)\n",
        "dur_encode = np_utils.to_categorical(network_output[:,1], num_classes=n_dur)\n",
        "\n",
        "# reshape data for LSTM input\n",
        "network_input = np.reshape(network_input,(n_patterns, sequence_length,2,1))\n",
        "\n",
        "\n",
        "input_layer = Input(shape = (sequence_length,2,))\n",
        "\n",
        "encoder = LSTM(512, return_sequences=True)(input_layer)\n",
        "\n",
        "x = Dropout(0.3)(encoder)\n",
        "x = LSTM(512, return_sequences=True)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = LSTM(512)(x)\n",
        "\n",
        "note_dense = Dense(256)(x)\n",
        "note_decoder = Dropout(0.3)(note_dense)\n",
        "note_out=Dense(n_notes, activation=\"softmax\")(note_decoder)\n",
        "\n",
        "dur_dense = Dense(256)(x)\n",
        "dur_dedcoder = Dropout(0.3)(dur_dense)\n",
        "dur_out=Dense(n_dur, activation=\"softmax\")(dur_dedcoder)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=[note_out,dur_out])\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "# train the model\n",
        "filepath = \"weights-improvement-v1-seq50-b64-{epoch:02d}-{loss:.4f}.hdf5\"    \n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath, monitor='loss', \n",
        "    verbose=0,        \n",
        "    save_best_only=True,        \n",
        "    mode='min'\n",
        ") \n",
        "callbacks_list = [checkpoint]     \n",
        "model.fit(network_input, [note_encode, dur_encode], \n",
        "          epochs=100, batch_size=64, \n",
        "          callbacks = callbacks_list)\n",
        "\n",
        "input_layer = Input(shape = (sequence_length,2,))\n",
        "\n",
        "encoder = LSTM(512, return_sequences=True)(input_layer)\n",
        "\n",
        "x = Dropout(0.3)(encoder)\n",
        "x = LSTM(512, return_sequences=True)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = LSTM(512)(x)\n",
        "\n",
        "note_dense = Dense(256)(x)\n",
        "note_decoder = Dropout(0.3)(note_dense)\n",
        "note_out=Dense(n_notes, activation=\"softmax\")(note_decoder)\n",
        "\n",
        "dur_dense = Dense(256)(x)\n",
        "dur_dedcoder = Dropout(0.3)(dur_dense)\n",
        "dur_out=Dense(n_dur, activation=\"softmax\")(dur_dedcoder)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=[note_out,dur_out])\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "# Load the weights to each node\n",
        "weight = ''\n",
        "model.load_weights(weight)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}